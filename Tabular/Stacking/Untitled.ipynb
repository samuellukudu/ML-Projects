{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb641ef-eed4-4302-85be-ae7738fe7203",
   "metadata": {},
   "source": [
    "# STACKING\n",
    "Stacking is a meta-learning technique that involves training a model to combine the predictions of several base models. The idea behind stacking is to leverage the strengths of different models by combining them into an ensemble that is more accurate and robust than any of the individual models.\n",
    "\n",
    "There are several stacking techniques that have been developed over the years, including:\n",
    "\n",
    "* Simple Stacking: This is the simplest form of stacking, where the predictions of the base models are combined using a linear regression model.\n",
    "\n",
    "* Bagging: This is a variation of stacking where the base models are trained on different subsets of the training data, and their predictions are combined using a simple majority vote.\n",
    "\n",
    "* Boosting: This is another variation of stacking where the base models are trained sequentially, with each model trying to correct the errors of the previous model.\n",
    "\n",
    "* Blending: This is a technique where the predictions of the base models are combined using a weighted average.\n",
    "\n",
    "* Stacking with Meta-Features: This is a technique where the predictions of the base models are combined with additional features that are derived from the training data.\n",
    "\n",
    "* Hierarchical Stacking: This is a technique where the base models are arranged in a hierarchical structure, with each level of the hierarchy combining the predictions of the models at the previous level.\n",
    "\n",
    "* Multilevel Stacking: This is a technique where the base models are combined using a series of intermediate models, with each intermediate model combining the predictions of the models at the previous level.\n",
    "\n",
    "## 1. SIMPLE STACKING\n",
    "This is the simplest form of stacking, where the predictions of the base models are combined using a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf8dcf0-90be-48b1-a9b8-fad0d178becd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.977\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# First-level models\n",
    "model1 = LogisticRegression(random_state=42)\n",
    "model2 = RandomForestClassifier(random_state=42)\n",
    "model3 = SVC(random_state=42, probability=True)\n",
    "\n",
    "# Create arrays to hold predictions for training and test sets\n",
    "train_predictions = np.zeros((X_train.shape[0], 3))\n",
    "test_predictions = np.zeros((X_test.shape[0], 3))\n",
    "\n",
    "# Use k-fold cross-validation to generate first-level predictions for each model\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for i, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "    X_train_fold = X_train[train_idx]\n",
    "    y_train_fold = y_train[train_idx]\n",
    "    X_val_fold = X_train[val_idx]\n",
    "\n",
    "    # Train each model on the current training fold\n",
    "    model1.fit(X_train_fold, y_train_fold)\n",
    "    model2.fit(X_train_fold, y_train_fold)\n",
    "    model3.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Generate predictions for the current validation fold\n",
    "    train_predictions[val_idx, 0] = model1.predict_proba(X_val_fold)[:, 1]\n",
    "    train_predictions[val_idx, 1] = model2.predict_proba(X_val_fold)[:, 1]\n",
    "    train_predictions[val_idx, 2] = model3.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "# Fit second-level model on first-level predictions\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(train_predictions, y_train)\n",
    "\n",
    "# Generate test set predictions using first-level models and second-level model\n",
    "test_predictions[:, 0] = model1.predict_proba(X_test)[:, 1]\n",
    "test_predictions[:, 1] = model2.predict_proba(X_test)[:, 1]\n",
    "test_predictions[:, 2] = model3.predict_proba(X_test)[:, 1]\n",
    "meta_predictions = meta_model.predict(test_predictions)\n",
    "\n",
    "# Evaluate accuracy of final predictions\n",
    "accuracy = accuracy_score(y_test, meta_predictions)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ae521-50b1-437b-bca2-82ea75035e59",
   "metadata": {},
   "source": [
    "## 2. BAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97cef9a1-2a46-489a-adf6-7f07d9a8cb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Define the base models\n",
    "base_models = [RandomForestClassifier(n_estimators=10, random_state=42),\n",
    "               SVC(random_state=42, probability=True),\n",
    "               LogisticRegression(random_state=42),\n",
    "               DecisionTreeClassifier(random_state=42),\n",
    "               HistGradientBoostingClassifier(random_state=42)]\n",
    "\n",
    "# Define the meta model\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_folds = 5\n",
    "\n",
    "# Split the data into training and test sets\n",
    "split_index = int(0.8 * len(X))\n",
    "X_train, y_train = X[:split_index], y[:split_index]\n",
    "X_test, y_test = X[split_index:], y[split_index:]\n",
    "\n",
    "# Define the K-Fold cross-validator\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Fit the base models using bagging\n",
    "bagged_preds = []\n",
    "for i, model in enumerate(base_models):\n",
    "    fold_preds = []\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        # Fit the model to the bagged training set\n",
    "        bag_indices = np.random.choice(train_index, size=int(0.8*len(train_index)), replace=True)\n",
    "        X_bag, y_bag = X_train[bag_indices], y_train[bag_indices]\n",
    "        model.fit(X_bag, y_bag)\n",
    "        \n",
    "        # Make predictions on the validation set\n",
    "        fold_preds.append(model.predict(X_val_fold))\n",
    "        \n",
    "    # Concatenate the predictions vertically to form a new feature set\n",
    "    fold_preds = np.concatenate(fold_preds, axis=0)\n",
    "    bagged_preds.append(fold_preds)\n",
    "    \n",
    "# Concatenate the base model predictions horizontally to form a new training set\n",
    "X_train_meta = np.concatenate(bagged_preds, axis=0).reshape((len(X_train),-1))\n",
    "\n",
    "# Fit the meta model using the stacked data\n",
    "meta_model.fit(X_train_meta, y_train)\n",
    "\n",
    "# Make predictions on the test data using the meta-model\n",
    "bagged_test_preds = []\n",
    "for model in base_models:\n",
    "    bagged_test_preds.append(model.predict(X_test))\n",
    "\n",
    "X_test_meta = np.concatenate(bagged_test_preds, axis=0).reshape((len(X_test), -1))\n",
    "test_preds = meta_model.predict(X_test_meta)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bae3db-127a-4781-a66f-4d2f79c3b033",
   "metadata": {},
   "source": [
    "## Why simple stacking outperforms bagging\n",
    "\n",
    "The difference in accuracy between bagging and simple stacking could be due to several reasons. Here are a few possibilities:\n",
    "\n",
    "* Data variability: Simple stacking is better suited to situations where there is significant variability in the data, meaning that different models may perform better on different parts of the data. Bagging is better suited to situations where the models are very similar and the data is relatively stable. It could be that the breast cancer dataset does not have enough variability to benefit from bagging.\n",
    "\n",
    "* Model selection: The choice of models used in the ensemble can greatly affect its performance. It could be that the models used in the bagging ensemble are not well-suited to the problem at hand, or that they are too similar to each other to provide any benefit.\n",
    "\n",
    "* Ensemble size: The size of the ensemble can also affect its performance. It could be that the bagging ensemble is too small to provide any benefit, or that the simple stacking ensemble is too large and prone to overfitting.\n",
    "\n",
    "* Hyperparameters: The hyperparameters of the models used in the ensemble can greatly affect its performance. It could be that the hyperparameters used in the bagging ensemble are not well-tuned, or that the simple stacking ensemble is overfitting due to hyperparameters that are too flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55f517b-e1e1-47a6-8834-5310c90328eb",
   "metadata": {},
   "source": [
    "## 3. Boosting\n",
    "I must point out that boosting is not a stacking technique, but rather a type of ensemble learning method that combines weak models to create a strong model. However, we can still implement it using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e93568c-5745-4609-9896-e064f27dcf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the base estimator\n",
    "# DecisionTreeClassifier(max_depth=1) is the default base classifer in 'AdaClassifier'\n",
    "base_estimator = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "\n",
    "# Create the AdaBoost classifier\n",
    "ada_boost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=10, random_state=42)\n",
    "\n",
    "# Fit the AdaBoost classifier on the training data\n",
    "ada_boost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = ada_boost.predict(X_test)\n",
    "\n",
    "\"\"\"\n",
    "In this code, we first load the breast cancer dataset and split it into training and testing sets. \n",
    "Then, we create a Decision Tree classifier as the base estimator and an AdaBoost classifier with 10 estimators. \n",
    "We fit the AdaBoost classifier on the training data and make predictions on the test data. \n",
    "Finally, we evaluate the model's accuracy.\n",
    "\"\"\"\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e708dca3-eb0c-41e3-95b6-7fb3b0069364",
   "metadata": {},
   "source": [
    "## 4. BLENDING\n",
    "Blending is another stacking technique that involves training multiple base models on the original training data, and then training a meta-model on the predictions made by the base models on a holdout validation set. The meta-model is then used to make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5741b0d-c158-40a8-90cf-136f71c64772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the base models\n",
    "models = [\n",
    "    LogisticRegression(random_state=42),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    KNeighborsClassifier(n_neighbors=3)\n",
    "]\n",
    "\n",
    "# Train the base models on the training set\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "# Create the validation set by making predictions on the training set\n",
    "X_val = np.zeros((len(X_train), len(models)))\n",
    "for i, model in enumerate(models):\n",
    "    X_val[:, i] = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Train the meta-model on the validation set\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(X_val, y_train)\n",
    "\n",
    "# Make predictions on the test data using the base models\n",
    "base_preds = np.zeros((len(X_test), len(models)))\n",
    "for i, model in enumerate(models):\n",
    "    base_preds[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Make predictions on the test data using the meta-model\n",
    "test_preds = meta_model.predict_proba(base_preds)[:, 1]\n",
    "\n",
    "# Evaluate the performance of the stacking model\n",
    "\n",
    "\"\"\"\n",
    "In this example, we define three base models: a logistic regression model, a decision tree model, \n",
    "and a k-nearest neighbors model. \n",
    "We then train each of these models on the training data. \n",
    "Next, we create a validation set by making predictions on the training data using each of the base models. \n",
    "We concatenate these predictions horizontally to create a new training set for the meta-model. \n",
    "We train a logistic regression meta-model on this new training set.\n",
    "\"\"\"\n",
    "acc = accuracy_score(y_test, test_preds.round())\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d80756-451b-4298-a1c9-82efc8d8de2c",
   "metadata": {},
   "source": [
    "## 5. STACKING WITH META FEATURES\n",
    "In stacking with meta-features, the meta-features are additional features that are engineered from the predictions of the base models on the training data, and are then used as input to the meta-model along with the original features. These meta-features are typically created using out-of-fold predictions from the base models on the training data, and can include statistics such as mean, standard deviation, and maximum/minimum of the predictions, or even the original features concatenated with the base model predictions. The idea is to give the meta-model additional information about the base model predictions that it can use to make more accurate final predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d652a07-93b2-4329-984a-d02f0c63a500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Define the base models\n",
    "base_models = [\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42)\n",
    "]\n",
    "\n",
    "# Define the meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "split = int(len(X) * 0.7)\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "# Train the base models using K-fold cross-validation\n",
    "k_folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "base_preds = []\n",
    "for model in base_models:\n",
    "    fold_preds = []\n",
    "    for train_idx, val_idx in k_folds.split(X_train, y_train):\n",
    "        X_train_fold, y_train_fold = X_train[train_idx], y_train[train_idx]\n",
    "        X_val_fold, y_val_fold = X_train[val_idx], y_train[val_idx]\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        fold_preds.append(model.predict_proba(X_val_fold))\n",
    "    base_preds.append(np.concatenate(fold_preds))\n",
    "\n",
    "# Concatenate the base model predictions horizontally to form a new training set\n",
    "X_train_meta = np.concatenate(base_preds, axis=1)\n",
    "\n",
    "# Train the meta-model on the new training set\n",
    "meta_model.fit(X_train_meta, y_train)\n",
    "\n",
    "# Make predictions on the test data using the base models\n",
    "base_test_preds = []\n",
    "for model in base_models:\n",
    "    model.fit(X_train, y_train)\n",
    "    base_test_preds.append(model.predict_proba(X_test))\n",
    "\n",
    "# Concatenate the base model test predictions horizontally to form a new test set\n",
    "X_test_meta = np.concatenate(base_test_preds, axis=1)\n",
    "\n",
    "# Make predictions on the test data using the meta-model\n",
    "test_preds = meta_model.predict(X_test_meta)\n",
    "\n",
    "# Calculate the accuracy of the stacking model\n",
    "\n",
    "\"\"\"\n",
    "In this example, we define two base models (Random Forest and Gradient Boosting), \n",
    "and a meta-model (Logistic Regression).\n",
    "We then split the data into training and testing sets, and train the base models using K-fold cross-validation. \n",
    "We concatenate the base model predictions horizontally to form a new training set, \n",
    "and train the meta-model on this new set.\n",
    "Finally, we make predictions on the test data using the base models, \n",
    "concatenate the predictions horizontally to form a new test set, \n",
    "and make predictions on this new set using the meta-model. \n",
    "The accuracy of the stacking model is calculated using the accuracy_score function from scikit-learn.\n",
    "\"\"\"\n",
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8795a4-ad72-461d-8fad-b8e7e1f7f75a",
   "metadata": {},
   "source": [
    "## 6. Hierarchical Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa652d3c-f402-47f6-8a7b-c9f67e9eddaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6228070175438597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# First level base models\n",
    "base_models1 = [\n",
    "    RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42)\n",
    "]\n",
    "\n",
    "# First level blending model\n",
    "blending_model1 = LogisticRegression()\n",
    "\n",
    "# Second level base models\n",
    "base_models2 = [\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
    "]\n",
    "\n",
    "# Second level blending model\n",
    "blending_model2 = LogisticRegression()\n",
    "\n",
    "# First level predictions\n",
    "level1_preds = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    preds = []\n",
    "    for model in base_models1:\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        preds.append(model.predict(X_val_fold))\n",
    "        \n",
    "    level1_preds.append(np.column_stack(preds))\n",
    "\n",
    "# Concatenate the level 1 predictions horizontally to form a new training set\n",
    "X_train_meta1 = np.concatenate(level1_preds, axis=1).reshape((len(X_train), -1))\n",
    "\n",
    "# Fit the blending model on the level 1 predictions\n",
    "blending_model1.fit(X_train_meta1, y_train)\n",
    "\n",
    "# Second level predictions\n",
    "level2_preds = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_idx, val_idx in kf.split(X_train_meta1):\n",
    "    X_train_fold, X_val_fold = X_train_meta1[train_idx], X_train_meta1[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    preds = []\n",
    "    for model in base_models2:\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        preds.append(model.predict(X_val_fold))\n",
    "        \n",
    "    level2_preds.append(np.column_stack(preds))\n",
    "\n",
    "# Concatenate the level 2 predictions horizontally to form a new training set\n",
    "X_train_meta2 = np.concatenate(level2_preds, axis=1).reshape((len(X_train), -1))\n",
    "\n",
    "# Fit the blending model on the level 2 predictions\n",
    "blending_model2.fit(X_train_meta2, y_train)\n",
    "\n",
    "# # Make predictions on the test data\n",
    "# level1_test_preds = []\n",
    "# for model in base_models1:\n",
    "#     model.fit(X_train, y_train)\n",
    "#     level1_test_preds.append(model.predict(X_test))\n",
    "\n",
    "# X_test_meta1 = np.column_stack(level1_test_preds)\n",
    "\n",
    "# test_preds = blending_model1.predict(X_test_meta1)\n",
    "\n",
    "# Predictions on the test data\n",
    "test_level_0 = np.empty((X_test.shape[0], len(base_models1)))\n",
    "for i, model in enumerate(base_models1):\n",
    "    test_level_0[:, i] = model.predict(X_test)\n",
    "    \n",
    "test_level_1 = np.empty((X_test.shape[0], len(base_models2)))\n",
    "for i, model in enumerate(base_models2):\n",
    "    test_level_1[:, i] = model.predict(test_level_0)\n",
    "\n",
    "# Meta model prediction\n",
    "# test_preds = blending_model2.predict(test_level_1)\n",
    "# blending_pred1 = blending_model1.predict_proba(test_x)[:, 1]\n",
    "# blending_pred2 = blending_model2.predict_proba(test_x)[:, 1]\n",
    "# test_preds = (blending_pred1 + blending_pred2) / 2\n",
    "\n",
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acbaef3-46f2-4c72-bebf-278ff3fb381c",
   "metadata": {},
   "source": [
    "## 7. Multilevel Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f908cced-f198-46a6-a5a2-1ad264938a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training level 0 model 1/4\n",
      "Training level 0 model 2/4\n",
      "Training level 0 model 3/4\n",
      "Training level 0 model 4/4\n",
      "Training level 1 model 1/4\n",
      "Training level 1 model 2/4\n",
      "Training level 1 model 3/4\n",
      "Training level 1 model 4/4\n",
      "Accuracy: 0.9630931458699473\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Initialize models for level 0\n",
    "level0_models = [\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    RandomForestClassifier(n_estimators=300, random_state=42),\n",
    "    RandomForestClassifier(n_estimators=400, random_state=42),\n",
    "]\n",
    "\n",
    "# Initialize models for level 1\n",
    "level1_models = [\n",
    "    LogisticRegression(random_state=42),\n",
    "    LogisticRegression(C=0.5, random_state=42),\n",
    "    LogisticRegression(C=0.8, random_state=42),\n",
    "    LogisticRegression(C=1.0, random_state=42),\n",
    "]\n",
    "\n",
    "# Initialize model for level 2 (meta-model)\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Split data into train and test sets\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize arrays to store base model predictions and meta features\n",
    "level0_train_preds = np.zeros((X.shape[0], len(level0_models)))\n",
    "level0_test_preds = np.zeros((X.shape[0], len(level0_models)))\n",
    "level1_train_preds = np.zeros((X.shape[0], len(level1_models)))\n",
    "level1_test_preds = np.zeros((X.shape[0], len(level1_models)))\n",
    "\n",
    "# Train base models and generate base model predictions for level 0\n",
    "for i, model in enumerate(level0_models):\n",
    "    print(f'Training level 0 model {i+1}/{len(level0_models)}')\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_val, y_val = X[val_idx], y[val_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Store base model predictions\n",
    "        level0_train_preds[val_idx, i] = model.predict(X_val)\n",
    "        level0_test_preds[:, i] += model.predict(X) / kf.n_splits\n",
    "\n",
    "# Generate meta features for level 1 using base model predictions from level 0\n",
    "for i, model in enumerate(level1_models):\n",
    "    print(f'Training level 1 model {i+1}/{len(level1_models)}')\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, y_train = level0_train_preds[train_idx], y[train_idx]\n",
    "        X_val, y_val = level0_train_preds[val_idx], y[val_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Store meta features\n",
    "        level1_train_preds[val_idx, i] = model.predict(X_val)\n",
    "        level1_test_preds[:, i] += model.predict(level0_test_preds) / kf.n_splits\n",
    "\n",
    "# Train meta-model using meta features from level 1\n",
    "meta_model.fit(level1_train_preds, y)\n",
    "\n",
    "# Make predictions on test data using meta-model and meta features from level 1\n",
    "test_preds = meta_model.predict(level1_test_preds)\n",
    "\n",
    "# Calculate accuracy score\n",
    "print(f'Accuracy: {accuracy_score(y, meta_model.predict(level1_train_preds))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c61b6-c165-428a-a3ef-4606a40b2f9c",
   "metadata": {},
   "source": [
    "# Inference\n",
    "If the test set is in a separate file, you can load it into a pandas DataFrame or a numpy array, and then preprocess it in the same way as you did for the training set. Once you have preprocessed the test set, you can use the base models trained in the first stage of the Multilevel Stacking to generate predictions for the test set. These predictions will then be used as input to the second stage of the stacking, where the meta-model will generate the final predictions.\n",
    "\n",
    "To use the pre-trained base models for prediction on the test set, you can simply call the predict method of each base model with the preprocessed test set as input. Once you have obtained the base model predictions for the test set, you can concatenate them horizontally and feed them to the meta-model for final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d65d56b6-9c55-48a5-af05-14546dc12344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test level 0 model 1/4\n",
      "Test level 0 model 2/4\n",
      "Test level 0 model 3/4\n",
      "Test level 0 model 4/4\n",
      "Test level 1 model 1/4\n",
      "Test level 1 model 2/4\n",
      "Test level 1 model 3/4\n",
      "Test level 1 model 4/4\n",
      "Accuracy: 0.9630931458699473\n"
     ]
    }
   ],
   "source": [
    "# base model 0\n",
    "test_level0 = np.zeros((X_test.shape[0], len(level0_models)))\n",
    "for i, model in enumerate(level0_models):\n",
    "    print(f'Test level 0 model {i+1}/{len(level0_models)}')\n",
    "    test_level0[:, i] += model.predict(X_test)\n",
    "    \n",
    "# base model 1\n",
    "test_level1 = np.zeros((X_test.shape[0], len(level1_models)))\n",
    "for i, model in enumerate(level1_models):\n",
    "    print(f'Test level 1 model {i+1}/{len(level1_models)}')\n",
    "    test_level1[:, i] += model.predict(test_level0)\n",
    "    \n",
    "    \n",
    "test_preds = meta_model.predict(test_level1)\n",
    "# Calculate accuracy score\n",
    "print(f'Accuracy: {accuracy_score(y, meta_model.predict(level1_train_preds))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b95472d-3f90-4edc-9ae1-ff7200681133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
